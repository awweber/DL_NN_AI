{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritt 1: Word2vec-Index herunterladen\n",
    "\n",
    "https://devmount.github.io/GermanWordEmbeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datei existiert bereits\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm \n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(\"german.model\"):\n",
    "\n",
    "    url = \"https://downloads.codingcoursestv.eu/037%20-%20neuronale%20netze/german.model.zip\"\n",
    "    # Streaming, so we can iterate over the response.\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Total size in bytes.\n",
    "    total_size = int(r.headers.get('content-length', 0));\n",
    "    \n",
    "    print(\"Downloading \" + str(total_size / 1024 / 1024) + \" MB.\")\n",
    "    \n",
    "    block_size = 1024\n",
    "    with open('german.model.zip', 'wb') as f:\n",
    "        for data in tqdm(r.iter_content(block_size), total=math.ceil(total_size//block_size), unit='KB', unit_divisor=1024, unit_scale=True):\n",
    "            f.write(data)\n",
    "            \n",
    "    # Extracting .zipFile\n",
    "    with zipfile.ZipFile(\"german.model.zip\", \"r\") as zipf:\n",
    "        zipf.extract(\"german.model\")\n",
    "        \n",
    "    # Remove .zip-File (we don't need it anymore)\n",
    "    os.remove(\"german.model.zip\")\n",
    "    \n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Datei existiert bereits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec (gensim): https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(\"german.model\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion des Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Putin', 0.7802654504776001),\n",
       " ('US-Praesident_Obama', 0.732889711856842),\n",
       " ('Barack_Obama', 0.7064037919044495),\n",
       " ('Russlands_Staatschef', 0.6975833177566528),\n",
       " ('Wladimir_Putin', 0.6830098628997803),\n",
       " ('russischen_Praesidenten', 0.6809293031692505),\n",
       " ('US-Praesident_Barack', 0.6808925271034241),\n",
       " ('US-Aussenminister_Kerry', 0.6703359484672546),\n",
       " ('Kremlchef_Wladimir', 0.6681345105171204),\n",
       " ('Russlands_Praesident', 0.6653517484664917)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obama - USA + Russland = Putin\n",
    "\n",
    "wv.most_similar(positive=[\"Obama\", \"Russland\"], negative=[\"USA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kanzlerin_Merkel', 0.6972562074661255),\n",
       " ('Merkel_CDU', 0.689968466758728),\n",
       " ('Kanzlerin', 0.6769890189170837),\n",
       " ('US-Praesident_Obama', 0.6683844327926636),\n",
       " ('Obama', 0.650946319103241),\n",
       " ('Russlands_Staatschef', 0.6507245898246765),\n",
       " ('Bundeskanzlerin', 0.6503757834434509),\n",
       " ('Ueberwachung_Handys', 0.6279087662696838),\n",
       " ('Angela_Merkel', 0.6278500556945801),\n",
       " ('Rande_G20-Gipfels', 0.6272603273391724)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=[\"Merkel\", \"USA\"], negative=[\"Deutschland\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grossmutter', 0.8721050024032593),\n",
       " ('Schwester', 0.8409271240234375),\n",
       " ('Tochter', 0.8314520716667175),\n",
       " ('Vater', 0.825255274772644),\n",
       " ('Tante', 0.8161165714263916),\n",
       " ('Oma', 0.8082747459411621),\n",
       " ('Frau', 0.7920665740966797),\n",
       " ('vierjaehrige_Tochter', 0.7915295362472534),\n",
       " ('Stiefmutter', 0.7876833081245422),\n",
       " ('Pflegemutter', 0.78443843126297)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=[\"Mutter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_dim = wv.vector_size\n",
    "vocab_size = len(wv.key_to_index)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for i, word in enumerate(wv.key_to_index):\n",
    "\tembedding_matrix[i] = wv[word]\n",
    "\n",
    "# Create Keras Embedding layer with pretrained weights\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
